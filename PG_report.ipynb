{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Policy Gradient Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is to solve CartPole problem from Gym OpenAI. Following is sample code of vanilla PG provided by https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.mtwpvfi8b. My understanding of each line of code is in the inline comments. \n",
    "\n",
    "To summarize, the sample code uses vanilla PG to directly compute the gradient of loss function: L = -A(s,a)*log(pi(s|a)) with respect to variables, i.e., the two layers \n",
    "in the network, and then use gradient ascension to update the network. The following image is the mean reward of latest 20 episodes over time. \n",
    "For each episode, the max reward is set to be 200.\n",
    "\n",
    "<img src=\"reward.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: build up PG agent. This agent is built with two layers of neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, lr, s_size,a_size,h_size):\n",
    "        #Create placeholder for states\n",
    "        self.state_in= tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n",
    "        #Create a hidden layer, with size of h_size\n",
    "        hidden = tf.contrib.slim.fully_connected(self.state_in,h_size,biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "        #Calculate output, with size of (# of episodes, # of actions).\n",
    "        self.output = tf.contrib.slim.fully_connected(hidden,a_size,activation_fn=tf.nn.softmax,biases_initializer=None)\n",
    "\n",
    "        #Feed the reward and chosen action into the network to compute the loss. \n",
    "        #The loss function is L = -A(s,a)*log(pi(s|a)). Then use it to update the network.\n",
    "        self.reward_holder = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        #Create a list of indices that point to each responsible action for each output.\n",
    "        self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
    "        #Select outputs by indicies.\n",
    "        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
    "        #Plug the responsible outputs, i.e., policy into the loss function along with the rewards.\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.reward_holder)\n",
    "        \n",
    "        #List all variables, i.e., the two layers of neural network.\n",
    "        tvars = tf.trainable_variables()\n",
    "        #The next four lines create gradient holders for later zip\n",
    "        self.gradient_holders = []\n",
    "        for idx,_ in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32,name=str(idx)+'_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "        \n",
    "        #Compute gradients with respect to each of two layers\n",
    "        self.gradients = tf.gradients(self.loss,tvars)\n",
    "        \n",
    "        #Use Adamn optimizer and apply the pair of gradient holder and gradient to the optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars), global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: train the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear the Tensorflow graph.\n",
    "tf.reset_default_graph() \n",
    "#The next two lines set the learning rate to decay over time\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.compat.v1.train.exponential_decay(0.01, global_step, 500, 0.9)\n",
    "\n",
    "#Load the agent.\n",
    "myAgent = agent(lr=learning_rate,s_size=4,a_size=2,h_size=8) \n",
    "#Set total number of episodes to train agent on.\n",
    "total_episodes = 5000 \n",
    "#Set max moves in each episodes\n",
    "max_ep = 999\n",
    "update_frequency = 5\n",
    "\n",
    "#Create a list for mean reward over time\n",
    "re = []\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    total_reward = []\n",
    "    total_length = []\n",
    "    \n",
    "    #The next three lines create a list for the gradients of two neural layers and set them to zero to ensure later update.\n",
    "    gradBuffer = sess.run(tf.trainable_variables())\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "\n",
    "    #Start training\n",
    "    while i < total_episodes:\n",
    "        #Get the initial state\n",
    "        s = env.reset()\n",
    "        running_reward = 0\n",
    "        #Create buffer of experience for later update\n",
    "        ep_history = []\n",
    "        for j in range(max_ep):\n",
    "            #Probabilistically pick an action given our network outputs.\n",
    "            a_dist = sess.run(myAgent.output,feed_dict={myAgent.state_in:[s]})\n",
    "            a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "            a = np.argmax(a_dist == a)\n",
    "\n",
    "            #Get our reward for taking an action.\n",
    "            s1,r,d,_ = env.step(a) \n",
    "            #Add this experience to buffer\n",
    "            ep_history.append([s,a,r,s1])\n",
    "            #Set the current state to next state\n",
    "            s = s1\n",
    "            running_reward += r\n",
    "            #If the episode is done, i.e, the pole fell or this episode has gained 200 rewards\n",
    "            if d == True:\n",
    "                ep_history = np.array(ep_history)\n",
    "                #Get the list of rewards\n",
    "                ep_history[:,2] = discount_rewards(ep_history[:,2])\n",
    "                #Feed the actions, rewards and states from experience buffer into the graph\n",
    "                feed_dict={myAgent.reward_holder:ep_history[:,2],\n",
    "                        myAgent.action_holder:ep_history[:,1],myAgent.state_in:np.vstack(ep_history[:,0])}\n",
    "                #Compute the gradients of the loss function with respect to the two neural layers\n",
    "                #Then update the gradient buffer created at the beginning\n",
    "                grads = sess.run([myAgent.gradients], feed_dict=feed_dict)\n",
    "                for idx,grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad\n",
    "\n",
    "                #If it is time to update the network, update it.\n",
    "                if i % update_frequency == 0 and i != 0:\n",
    "                    #Feed gradient holders and the gradient buffer to the graph\n",
    "                    feed_dict= dictionary = dict(zip(myAgent.gradient_holders, gradBuffer))\n",
    "                    sess.run(myAgent.update_batch, feed_dict=feed_dict)\n",
    "                    #Clear gradient buffer for next episode\n",
    "                    for ix,grad in enumerate(gradBuffer):\n",
    "                        gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                total_reward.append(running_reward)\n",
    "                total_length.append(j)\n",
    "                break\n",
    "\n",
    "        \n",
    "        #Update our running tally of scores.\n",
    "        if i % 100 == 0:\n",
    "            print(np.mean(total_reward[-25:]))\n",
    "            re.append(np.mean(total_reward[-25:]))\n",
    "        i += 1"
   ]
  }
 ]
}